"""
Laniakea Protocol - Machine Learning System
Ø³ÛŒØ³ØªÙ… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
"""

import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from time import time
import json


@dataclass
class TrainingData:
    """Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ"""
    features: np.ndarray
    labels: np.ndarray
    timestamp: float


class NeuralNetwork:
    """
    Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§
    """
    
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """
        Args:
            input_size: ØªØ¹Ø¯Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§
            hidden_size: ØªØ¹Ø¯Ø§Ø¯ Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù…Ø®ÙÛŒ
            output_size: ØªØ¹Ø¯Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§
        """
        # ÙˆØ²Ù†â€ŒÙ‡Ø§
        self.w1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.w2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¢Ù…ÙˆØ²Ø´
        self.training_history: List[float] = []
        
        print(f"ğŸ§  Neural Network initialized: {input_size}-{hidden_size}-{output_size}")
    
    def sigmoid(self, x: np.ndarray) -> np.ndarray:
        """ØªØ§Ø¨Ø¹ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ sigmoid"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:
        """Ù…Ø´ØªÙ‚ sigmoid"""
        return x * (1 - x)
    
    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Ù¾ÛŒØ´â€ŒØ®ÙˆØ±
        
        Args:
            X: ÙˆØ±ÙˆØ¯ÛŒ
        
        Returns:
            (hidden_output, final_output)
        """
        # Ù„Ø§ÛŒÙ‡ Ù…Ø®ÙÛŒ
        z1 = np.dot(X, self.w1) + self.b1
        a1 = self.sigmoid(z1)
        
        # Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        z2 = np.dot(a1, self.w2) + self.b2
        a2 = self.sigmoid(z2)
        
        return a1, a2
    
    def backward(
        self,
        X: np.ndarray,
        y: np.ndarray,
        a1: np.ndarray,
        a2: np.ndarray,
        learning_rate: float = 0.01
    ):
        """
        Ù¾Ø³â€ŒØ§Ù†ØªØ´Ø§Ø± Ø®Ø·Ø§
        
        Args:
            X: ÙˆØ±ÙˆØ¯ÛŒ
            y: Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ
            a1: Ø®Ø±ÙˆØ¬ÛŒ Ù„Ø§ÛŒÙ‡ Ù…Ø®ÙÛŒ
            a2: Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            learning_rate: Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        """
        m = X.shape[0]
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§
        error = a2 - y
        
        # Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
        d2 = error * self.sigmoid_derivative(a2)
        dw2 = np.dot(a1.T, d2) / m
        db2 = np.sum(d2, axis=0, keepdims=True) / m
        
        # Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù„Ø§ÛŒÙ‡ Ù…Ø®ÙÛŒ
        d1 = np.dot(d2, self.w2.T) * self.sigmoid_derivative(a1)
        dw1 = np.dot(X.T, d1) / m
        db1 = np.sum(d1, axis=0, keepdims=True) / m
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§
        self.w2 -= learning_rate * dw2
        self.b2 -= learning_rate * db2
        self.w1 -= learning_rate * dw1
        self.b1 -= learning_rate * db1
    
    def train(
        self,
        X: np.ndarray,
        y: np.ndarray,
        epochs: int = 1000,
        learning_rate: float = 0.01,
        verbose: bool = False
    ):
        """
        Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡
        
        Args:
            X: Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ
            y: Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
            epochs: ØªØ¹Ø¯Ø§Ø¯ epoch
            learning_rate: Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
            verbose: Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
        """
        for epoch in range(epochs):
            # Ù¾ÛŒØ´â€ŒØ®ÙˆØ±
            a1, a2 = self.forward(X)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss
            loss = np.mean((a2 - y) ** 2)
            self.training_history.append(loss)
            
            # Ù¾Ø³â€ŒØ§Ù†ØªØ´Ø§Ø±
            self.backward(X, y, a1, a2, learning_rate)
            
            if verbose and (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        
        Args:
            X: ÙˆØ±ÙˆØ¯ÛŒ
        
        Returns:
            Ø®Ø±ÙˆØ¬ÛŒ
        """
        _, output = self.forward(X)
        return output
    
    def save_weights(self) -> Dict:
        """Ø°Ø®ÛŒØ±Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§"""
        return {
            "w1": self.w1.tolist(),
            "b1": self.b1.tolist(),
            "w2": self.w2.tolist(),
            "b2": self.b2.tolist()
        }
    
    def load_weights(self, weights: Dict):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§"""
        self.w1 = np.array(weights["w1"])
        self.b1 = np.array(weights["b1"])
        self.w2 = np.array(weights["w2"])
        self.b2 = np.array(weights["b2"])


class ValuePredictor:
    """
    Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø§Ø±Ø²Ø´ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§
    
    Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø§Ø±Ø²Ø´ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """
    
    def __init__(self):
        # Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¨ÙØ¹Ø¯
        self.models: Dict[str, NeuralNetwork] = {
            "knowledge": NeuralNetwork(10, 20, 1),
            "computation": NeuralNetwork(10, 20, 1),
            "originality": NeuralNetwork(10, 20, 1),
            "consciousness": NeuralNetwork(10, 20, 1)
        }
        
        self.training_data: List[TrainingData] = []
        
        print("ğŸ”® Value Predictor initialized")
    
    def extract_features(self, solution_text: str, task_difficulty: float) -> np.ndarray:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø±Ø§Ù‡â€ŒØ­Ù„
        
        Args:
            solution_text: Ù…ØªÙ† Ø±Ø§Ù‡â€ŒØ­Ù„
            task_difficulty: Ø¯Ø´ÙˆØ§Ø±ÛŒ ØªØ³Ú©
        
        Returns:
            Ø¨Ø±Ø¯Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒ
        """
        features = []
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†
        features.append(len(solution_text))  # Ø·ÙˆÙ„
        features.append(len(solution_text.split()))  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª
        features.append(len(set(solution_text.split())))  # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª ÛŒÚ©ØªØ§
        features.append(solution_text.count('\n'))  # ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ·
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ØªÙˆØ§
        features.append(float('math' in solution_text.lower()))
        features.append(float('algorithm' in solution_text.lower()))
        features.append(float('theory' in solution_text.lower()))
        features.append(float('proof' in solution_text.lower()))
        
        # ÙˆÛŒÚ˜Ú¯ÛŒ ØªØ³Ú©
        features.append(task_difficulty)
        features.append(task_difficulty ** 2)
        
        return np.array(features).reshape(1, -1)
    
    def train_on_solution(
        self,
        solution_text: str,
        task_difficulty: float,
        actual_values: Dict[str, float]
    ):
        """
        Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒÚ© Ø±Ø§Ù‡â€ŒØ­Ù„
        
        Args:
            solution_text: Ù…ØªÙ† Ø±Ø§Ù‡â€ŒØ­Ù„
            task_difficulty: Ø¯Ø´ÙˆØ§Ø±ÛŒ
            actual_values: Ø§Ø±Ø²Ø´â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
        """
        features = self.extract_features(solution_text, task_difficulty)
        
        # Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø± Ù…Ø¯Ù„
        for dimension, value in actual_values.items():
            if dimension in self.models:
                labels = np.array([[value / 100.0]])  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
                self.models[dimension].train(features, labels, epochs=10, verbose=False)
    
    def predict_value(
        self,
        solution_text: str,
        task_difficulty: float
    ) -> Dict[str, float]:
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø±Ø²Ø´ Ø±Ø§Ù‡â€ŒØ­Ù„
        
        Args:
            solution_text: Ù…ØªÙ† Ø±Ø§Ù‡â€ŒØ­Ù„
            task_difficulty: Ø¯Ø´ÙˆØ§Ø±ÛŒ
        
        Returns:
            Ø§Ø±Ø²Ø´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡
        """
        features = self.extract_features(solution_text, task_difficulty)
        
        predictions = {}
        for dimension, model in self.models.items():
            pred = model.predict(features)[0, 0]
            predictions[dimension] = float(pred * 100.0)  # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³ Ø§ØµÙ„ÛŒ
        
        return predictions
    
    def get_model_stats(self) -> Dict:
        """Ø¢Ù…Ø§Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        stats = {}
        for dimension, model in self.models.items():
            if model.training_history:
                stats[dimension] = {
                    "training_iterations": len(model.training_history),
                    "final_loss": model.training_history[-1],
                    "initial_loss": model.training_history[0]
                }
        return stats


class PatternRecognizer:
    """
    ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯Ùˆ Ø¯Ø± Ø¨Ù„Ø§Ú©â€ŒÚ†ÛŒÙ†
    
    Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯ Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
    - Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
    - Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø±Ø²Ø´ÛŒ
    - Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ
    """
    
    def __init__(self):
        self.patterns: Dict[str, List] = {
            "temporal": [],
            "value": [],
            "user": []
        }
        
        print("ğŸ” Pattern Recognizer initialized")
    
    def analyze_temporal_patterns(self, blocks: List) -> Dict:
        """
        ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
        
        Args:
            blocks: Ù„ÛŒØ³Øª Ø¨Ù„Ø§Ú©â€ŒÙ‡Ø§
        
        Returns:
            Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡
        """
        if len(blocks) < 10:
            return {"message": "Not enough data"}
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…Ø§Ù†â€ŒÙ‡Ø§
        timestamps = [b.timestamp for b in blocks]
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ
        intervals = np.diff(timestamps)
        
        patterns = {
            "average_block_time": float(np.mean(intervals)),
            "std_block_time": float(np.std(intervals)),
            "min_block_time": float(np.min(intervals)),
            "max_block_time": float(np.max(intervals))
        }
        
        return patterns
    
    def analyze_value_patterns(self, blocks: List) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø±Ø²Ø´ÛŒ"""
        if not blocks:
            return {}
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø±Ø²Ø´â€ŒÙ‡Ø§
        total_values = []
        for block in blocks:
            if hasattr(block, 'solution') and block.solution:
                total_values.append(block.solution.value_vector.total_value())
        
        if not total_values:
            return {"message": "No value data"}
        
        values_array = np.array(total_values)
        
        return {
            "average_value": float(np.mean(values_array)),
            "std_value": float(np.std(values_array)),
            "max_value": float(np.max(values_array)),
            "trend": "increasing" if len(values_array) > 1 and values_array[-1] > values_array[0] else "stable"
        }
    
    def detect_anomalies(self, data: np.ndarray, threshold: float = 2.0) -> List[int]:
        """
        ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ
        
        Args:
            data: Ø¯Ø§Ø¯Ù‡
            threshold: Ø¢Ø³ØªØ§Ù†Ù‡ (Ú†Ù†Ø¯ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±)
        
        Returns:
            Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±
        """
        mean = np.mean(data)
        std = np.std(data)
        
        anomalies = []
        for i, value in enumerate(data):
            z_score = abs((value - mean) / std) if std > 0 else 0
            if z_score > threshold:
                anomalies.append(i)
        
        return anomalies


class ReinforcementLearner:
    """
    ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ
    
    ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú†Ù‡ ØªØµÙ…ÛŒÙ…Ø§ØªÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯
    """
    
    def __init__(self, n_actions: int):
        """
        Args:
            n_actions: ØªØ¹Ø¯Ø§Ø¯ Ø§Ø¹Ù…Ø§Ù„ Ù…Ù…Ú©Ù†
        """
        self.n_actions = n_actions
        self.q_table: Dict[str, np.ndarray] = {}  # state -> Q values
        
        # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        self.alpha = 0.1  # Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self.gamma = 0.9  # Ø¶Ø±ÛŒØ¨ ØªØ®ÙÛŒÙ
        self.epsilon = 0.1  # Ù†Ø±Ø® Ø§Ú©ØªØ´Ø§Ù
        
        print(f"ğŸ® Reinforcement Learner initialized with {n_actions} actions")
    
    def get_q_values(self, state: str) -> np.ndarray:
        """Ø¯Ø±ÛŒØ§ÙØª Q values Ø¨Ø±Ø§ÛŒ ÛŒÚ© state"""
        if state not in self.q_table:
            self.q_table[state] = np.zeros(self.n_actions)
        return self.q_table[state]
    
    def choose_action(self, state: str) -> int:
        """
        Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù…Ù„ (epsilon-greedy)
        
        Args:
            state: ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ
        
        Returns:
            Ø´Ù…Ø§Ø±Ù‡ Ø¹Ù…Ù„
        """
        if np.random.random() < self.epsilon:
            # Ø§Ú©ØªØ´Ø§Ù
            return np.random.randint(self.n_actions)
        else:
            # Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ
            q_values = self.get_q_values(state)
            return int(np.argmax(q_values))
    
    def update(
        self,
        state: str,
        action: int,
        reward: float,
        next_state: str
    ):
        """
        Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Q-table
        
        Args:
            state: ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ
            action: Ø¹Ù…Ù„ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡
            reward: Ù¾Ø§Ø¯Ø§Ø´
            next_state: ÙˆØ¶Ø¹ÛŒØª Ø¨Ø¹Ø¯ÛŒ
        """
        current_q = self.get_q_values(state)[action]
        next_max_q = np.max(self.get_q_values(next_state))
        
        # Q-learning update
        new_q = current_q + self.alpha * (reward + self.gamma * next_max_q - current_q)
        
        self.q_table[state][action] = new_q
    
    def get_stats(self) -> Dict:
        """Ø¢Ù…Ø§Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        return {
            "states_explored": len(self.q_table),
            "total_q_values": sum(len(q) for q in self.q_table.values()),
            "epsilon": self.epsilon
        }


class MLOrchestrator:
    """
    Ù‡Ù…Ø§Ù‡Ù†Ú¯â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ML
    
    ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§ÛŒ ML Ø±Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ù‡Ù…Ø§Ù‡Ù†Ú¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    """
    
    def __init__(self):
        self.value_predictor = ValuePredictor()
        self.pattern_recognizer = PatternRecognizer()
        self.rl_learner = ReinforcementLearner(n_actions=5)
        
        print("ğŸ¯ ML Orchestrator initialized")
    
    def analyze_blockchain(self, blocks: List) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¨Ù„Ø§Ú©â€ŒÚ†ÛŒÙ†"""
        return {
            "temporal_patterns": self.pattern_recognizer.analyze_temporal_patterns(blocks),
            "value_patterns": self.pattern_recognizer.analyze_value_patterns(blocks)
        }
    
    def get_stats(self) -> Dict:
        """Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„ ML"""
        return {
            "value_predictor": self.value_predictor.get_model_stats(),
            "rl_learner": self.rl_learner.get_stats(),
            "pattern_recognizer": {
                "patterns_found": sum(len(p) for p in self.pattern_recognizer.patterns.values())
            }
        }
