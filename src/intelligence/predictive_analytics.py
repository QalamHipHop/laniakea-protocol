"""
Predictive Analytics - ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÚ©Ù†Ù†Ø¯Ù‡
Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÙ†Ø¯Ù‡Ø§ØŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ùˆ Ø¢ÛŒÙ†Ø¯Ù‡ Ø´Ø¨Ú©Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ML Ùˆ AI
"""

import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
import json

from openai import OpenAI


class TrendAnalyzer:
    """ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø±ÙˆÙ†Ø¯Ù‡Ø§"""

    def __init__(self):
        self.data_points = defaultdict(list)
        self.predictions = {}

    def add_data_point(self, metric: str, value: float, timestamp: Optional[datetime] = None):
        """Ø§ÙØ²ÙˆØ¯Ù† Ù†Ù‚Ø·Ù‡ Ø¯Ø§Ø¯Ù‡"""
        if timestamp is None:
            timestamp = datetime.now()

        self.data_points[metric].append({"value": value, "timestamp": timestamp.isoformat()})

    def calculate_trend(self, metric: str, window_size: int = 10) -> Dict[str, Any]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯"""

        if metric not in self.data_points or len(self.data_points[metric]) < 2:
            return {"trend": "insufficient_data"}

        recent_data = self.data_points[metric][-window_size:]
        values = [d["value"] for d in recent_data]

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ÙˆÙ†Ø¯ Ø®Ø·ÛŒ Ø³Ø§Ø¯Ù‡
        n = len(values)
        x = np.arange(n)
        y = np.array(values)

        # Least squares regression
        slope = (n * np.sum(x * y) - np.sum(x) * np.sum(y)) / (n * np.sum(x**2) - np.sum(x) ** 2)
        intercept = (np.sum(y) - slope * np.sum(x)) / n

        # ØªØ¹ÛŒÛŒÙ† Ø¬Ù‡Øª Ø±ÙˆÙ†Ø¯
        if slope > 0.1:
            trend_direction = "increasing"
        elif slope < -0.1:
            trend_direction = "decreasing"
        else:
            trend_direction = "stable"

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ RÂ²
        y_pred = slope * x + intercept
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

        return {
            "metric": metric,
            "trend": trend_direction,
            "slope": float(slope),
            "r_squared": float(r_squared),
            "current_value": values[-1],
            "avg_value": float(np.mean(values)),
            "volatility": float(np.std(values)),
            "data_points": len(values),
        }

    def predict_next_value(self, metric: str, steps_ahead: int = 1) -> Optional[float]:
        """Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø¹Ø¯ÛŒ"""

        trend = self.calculate_trend(metric)

        if trend.get("trend") == "insufficient_data":
            return None

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ÙˆÙ†Ø¯
        predicted = trend["current_value"] + (trend["slope"] * steps_ahead)

        return max(0, predicted)  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ


class PatternRecognizer:
    """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""

    def __init__(self):
        self.known_patterns = {
            "spike": self._detect_spike,
            "drop": self._detect_drop,
            "cycle": self._detect_cycle,
            "anomaly": self._detect_anomaly,
        }

    def detect_patterns(self, data: List[float]) -> List[Dict[str, Any]]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªÙ…Ø§Ù… Ø§Ù„Ú¯ÙˆÙ‡Ø§"""

        if len(data) < 5:
            return []

        patterns = []

        for pattern_name, detector in self.known_patterns.items():
            result = detector(data)
            if result:
                patterns.append({"pattern": pattern_name, **result})

        return patterns

    def _detect_spike(self, data: List[float]) -> Optional[Dict[str, Any]]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ spike (Ø§ÙØ²Ø§ÛŒØ´ Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ)"""

        mean = np.mean(data)
        std = np.std(data)

        for i in range(len(data) - 1):
            if data[i + 1] > mean + 2 * std and data[i] < mean + std:
                return {"position": i + 1, "magnitude": data[i + 1] - data[i], "confidence": 0.8}

        return None

    def _detect_drop(self, data: List[float]) -> Optional[Dict[str, Any]]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ drop (Ú©Ø§Ù‡Ø´ Ù†Ø§Ú¯Ù‡Ø§Ù†ÛŒ)"""

        mean = np.mean(data)
        std = np.std(data)

        for i in range(len(data) - 1):
            if data[i + 1] < mean - 2 * std and data[i] > mean - std:
                return {"position": i + 1, "magnitude": data[i] - data[i + 1], "confidence": 0.8}

        return None

    def _detect_cycle(self, data: List[float]) -> Optional[Dict[str, Any]]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú†Ø±Ø®Ù‡ ØªÚ©Ø±Ø§Ø±ÛŒ"""

        if len(data) < 10:
            return None

        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ù„Ú¯ÙˆÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø³Ø§Ø¯Ù‡
        for period in range(2, len(data) // 2):
            correlation = 0
            count = 0

            for i in range(len(data) - period):
                if abs(data[i] - data[i + period]) < np.std(data) * 0.5:
                    correlation += 1
                count += 1

            if correlation / count > 0.7:
                return {"period": period, "confidence": correlation / count}

        return None

    def _detect_anomaly(self, data: List[float]) -> Optional[Dict[str, Any]]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ"""

        mean = np.mean(data)
        std = np.std(data)

        anomalies = []

        for i, value in enumerate(data):
            z_score = abs((value - mean) / std) if std > 0 else 0

            if z_score > 3:
                anomalies.append({"position": i, "value": value, "z_score": z_score})

        if anomalies:
            return {
                "count": len(anomalies),
                "positions": [a["position"] for a in anomalies],
                "confidence": 0.9,
            }

        return None


class PredictiveEngine:
    """Ù…ÙˆØªÙˆØ± Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ AI"""

    def __init__(self):
        self.client = OpenAI()
        self.trend_analyzer = TrendAnalyzer()
        self.pattern_recognizer = PatternRecognizer()
        self.predictions_history = []

    async def analyze_blockchain_future(
        self, blockchain_data: Dict[str, Any], network_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ Ø¨Ù„Ø§Ú©â€ŒÚ†ÛŒÙ†"""

        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ
        historical_summary = {
            "blockchain": blockchain_data,
            "network": network_data,
            "timestamp": datetime.now().isoformat(),
        }

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¨Ø§ AI
        response = self.client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "system",
                    "content": """You are a blockchain analytics expert specializing in predictive analysis.
Analyze trends and provide actionable insights about the future state of the network.""",
                },
                {
                    "role": "user",
                    "content": f"""Analyze this blockchain network data and predict future trends:

Current State:
{json.dumps(historical_summary, indent=2)}

Provide predictions for:
1. Network growth trajectory (next 7 days)
2. Potential bottlenecks or issues
3. Recommended optimizations
4. Value creation opportunities

Format as JSON with keys: growth_prediction, risks, optimizations, opportunities""",
                },
            ],
            temperature=0.7,
            max_tokens=1500,
        )

        ai_analysis = response.choices[0].message.content

        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ parse Ú©Ø±Ø¯Ù† JSON
        try:
            predictions = json.loads(ai_analysis)
        except json.JSONDecodeError:
            predictions = {"raw_analysis": ai_analysis, "parsed": False}

        # Ø§ÙØ²ÙˆØ¯Ù† ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ
        predictions["statistical_trends"] = self._calculate_statistical_predictions(
            blockchain_data, network_data
        )

        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
        self.predictions_history.append(
            {"timestamp": datetime.now().isoformat(), "predictions": predictions}
        )

        return predictions

    def _calculate_statistical_predictions(
        self, blockchain_data: Dict[str, Any], network_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ"""

        predictions = {}

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±Ø´Ø¯ Ø²Ù†Ø¬ÛŒØ±Ù‡
        if "chain_length" in blockchain_data:
            self.trend_analyzer.add_data_point("chain_length", blockchain_data["chain_length"])
            predictions["chain_length_trend"] = self.trend_analyzer.calculate_trend("chain_length")
            predictions["predicted_chain_length_7d"] = self.trend_analyzer.predict_next_value(
                "chain_length", steps_ahead=7
            )

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù‡Ù…ØªØ§ÛŒØ§Ù†
        if "peer_count" in network_data:
            self.trend_analyzer.add_data_point("peer_count", network_data["peer_count"])
            predictions["peer_count_trend"] = self.trend_analyzer.calculate_trend("peer_count")

        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ø±Ø²Ø´ Ú©Ù„
        if "total_value" in blockchain_data:
            self.trend_analyzer.add_data_point("total_value", blockchain_data["total_value"])
            predictions["value_trend"] = self.trend_analyzer.calculate_trend("total_value")

        return predictions

    def detect_emerging_patterns(self, metric_name: str, data: List[float]) -> Dict[str, Any]:
        """Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†ÙˆØ¸Ù‡ÙˆØ±"""

        patterns = self.pattern_recognizer.detect_patterns(data)

        return {
            "metric": metric_name,
            "patterns_found": len(patterns),
            "patterns": patterns,
            "timestamp": datetime.now().isoformat(),
        }

    def generate_forecast_report(self) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ"""

        if not self.predictions_history:
            return "No predictions available yet."

        latest = self.predictions_history[-1]

        report = f"""
# ğŸ”® Laniakea Protocol - Predictive Analytics Report

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary

Based on advanced AI analysis and statistical modeling, here are the key predictions for the Laniakea Protocol network:

## Growth Predictions

{json.dumps(latest['predictions'].get('growth_prediction', {}), indent=2)}

## Identified Risks

{json.dumps(latest['predictions'].get('risks', {}), indent=2)}

## Recommended Optimizations

{json.dumps(latest['predictions'].get('optimizations', {}), indent=2)}

## Value Creation Opportunities

{json.dumps(latest['predictions'].get('opportunities', {}), indent=2)}

## Statistical Trends

{json.dumps(latest['predictions'].get('statistical_trends', {}), indent=2)}

---

*This report is generated by Laniakea's Predictive Analytics Engine*
"""

        return report


# Global instance
_predictive_engine = PredictiveEngine()


def get_predictive_engine() -> PredictiveEngine:
    """Ø¯Ø±ÛŒØ§ÙØª instance Ø¬Ù‡Ø§Ù†ÛŒ"""
    return _predictive_engine
