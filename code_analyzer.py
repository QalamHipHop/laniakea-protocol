#!/usr/bin/env python3
"""
Laniakea Protocol - Automated Code Analysis Tool
ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ¯Ú©Ø§Ø± Ú©Ø¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ù†ÙˆØ§Ù‚Øµ
"""

import os
import ast
import json
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict

class CodeAnalyzer:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.issues = defaultdict(list)
        self.stats = defaultdict(int)
        
    def analyze_python_file(self, filepath: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÛŒÙ„ ÛŒÚ© ÙØ§ÛŒÙ„ Ù¾Ø§ÛŒØªÙˆÙ†"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # Parse AST
            tree = ast.parse(content, filename=str(filepath))
            
            file_issues = {
                'imports': [],
                'functions': [],
                'classes': [],
                'todos': [],
                'complexity': 0,
                'lines': len(content.split('\n'))
            }
            
            # ØªØ­Ù„ÛŒÙ„ imports
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        file_issues['imports'].append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        file_issues['imports'].append(node.module)
                        
                # ØªØ­Ù„ÛŒÙ„ functions
                elif isinstance(node, ast.FunctionDef):
                    file_issues['functions'].append({
                        'name': node.name,
                        'lineno': node.lineno,
                        'args': len(node.args.args),
                        'has_docstring': ast.get_docstring(node) is not None
                    })
                    
                # ØªØ­Ù„ÛŒÙ„ classes
                elif isinstance(node, ast.ClassDef):
                    file_issues['classes'].append({
                        'name': node.name,
                        'lineno': node.lineno,
                        'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                        'has_docstring': ast.get_docstring(node) is not None
                    })
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ TODO Ùˆ FIXME
            for i, line in enumerate(content.split('\n'), 1):
                if 'TODO' in line or 'FIXME' in line or 'XXX' in line:
                    file_issues['todos'].append({
                        'line': i,
                        'content': line.strip()
                    })
                    
            return file_issues
            
        except SyntaxError as e:
            self.issues['syntax_errors'].append({
                'file': str(filepath),
                'error': str(e)
            })
            return None
        except Exception as e:
            self.issues['parse_errors'].append({
                'file': str(filepath),
                'error': str(e)
            })
            return None
    
    def check_missing_files(self):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ú©Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù†Ø¯"""
        essential_files = [
            '.env.example',
            'pytest.ini',
            '.gitignore',
            'docker-compose.yml',
            'Dockerfile'
        ]
        
        for filename in essential_files:
            filepath = self.project_root / filename
            if not filepath.exists():
                self.issues['missing_files'].append(filename)
            else:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª
                if filepath.stat().st_size == 0:
                    self.issues['empty_files'].append(filename)
    
    def analyze_imports(self, all_files_data: Dict):
        """ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ import"""
        all_imports = defaultdict(int)
        
        for filepath, data in all_files_data.items():
            if data and 'imports' in data:
                for imp in data['imports']:
                    all_imports[imp] += 1
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† imports Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡ ÛŒØ§ Ú©Ù…ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡
        rarely_used = {k: v for k, v in all_imports.items() if v == 1}
        
        return {
            'total_unique_imports': len(all_imports),
            'rarely_used_imports': rarely_used,
            'most_used_imports': dict(sorted(all_imports.items(), key=lambda x: x[1], reverse=True)[:10])
        }
    
    def check_documentation(self, all_files_data: Dict):
        """Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        undocumented_functions = []
        undocumented_classes = []
        
        for filepath, data in all_files_data.items():
            if not data:
                continue
                
            for func in data.get('functions', []):
                if not func['has_docstring']:
                    undocumented_functions.append({
                        'file': filepath,
                        'function': func['name'],
                        'line': func['lineno']
                    })
            
            for cls in data.get('classes', []):
                if not cls['has_docstring']:
                    undocumented_classes.append({
                        'file': filepath,
                        'class': cls['name'],
                        'line': cls['lineno']
                    })
        
        return {
            'undocumented_functions': undocumented_functions[:20],  # ÙÙ‚Ø· 20 Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ„
            'undocumented_classes': undocumented_classes,
            'total_undocumented_functions': len(undocumented_functions),
            'total_undocumented_classes': len(undocumented_classes)
        }
    
    def analyze_project(self):
        """ØªØ­Ù„ÛŒÙ„ Ú©Ù„ Ù¾Ø±ÙˆÚ˜Ù‡"""
        print("ğŸ” Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù¾Ø±ÙˆÚ˜Ù‡ Laniakea Protocol...")
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†
        python_files = list(self.project_root.rglob('*.py'))
        python_files = [f for f in python_files if '.git' not in str(f) and 'venv' not in str(f)]
        
        print(f"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ† ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(python_files)}")
        
        # ØªØ­Ù„ÛŒÙ„ Ù‡Ø± ÙØ§ÛŒÙ„
        all_files_data = {}
        for filepath in python_files:
            rel_path = filepath.relative_to(self.project_root)
            data = self.analyze_python_file(filepath)
            if data:
                all_files_data[str(rel_path)] = data
                self.stats['total_lines'] += data['lines']
                self.stats['total_functions'] += len(data['functions'])
                self.stats['total_classes'] += len(data['classes'])
        
        print(f"âœ… ØªØ­Ù„ÛŒÙ„ {len(all_files_data)} ÙØ§ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯")
        
        # Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        self.check_missing_files()
        
        # ØªØ­Ù„ÛŒÙ„ imports
        import_analysis = self.analyze_imports(all_files_data)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø³ØªÙ†Ø¯Ø§Øª
        doc_analysis = self.check_documentation(all_files_data)
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†ØªØ§ÛŒØ¬
        results = {
            'summary': {
                'total_python_files': len(python_files),
                'successfully_analyzed': len(all_files_data),
                'total_lines_of_code': self.stats['total_lines'],
                'total_functions': self.stats['total_functions'],
                'total_classes': self.stats['total_classes']
            },
            'issues': dict(self.issues),
            'import_analysis': import_analysis,
            'documentation_analysis': doc_analysis,
            'files_data': all_files_data
        }
        
        return results

def main():
    analyzer = CodeAnalyzer('/home/ubuntu/laniakea-protocol')
    results = analyzer.analyze_project()
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    output_file = '/home/ubuntu/laniakea-protocol/code_analysis_report.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø¯Ø± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_file}")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
    print("\n" + "="*60)
    print("ğŸ“ˆ Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„:")
    print("="*60)
    print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØªÙˆÙ†: {results['summary']['total_python_files']}")
    print(f"âœ“ Ø®Ø·ÙˆØ· Ú©Ø¯: {results['summary']['total_lines_of_code']:,}")
    print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆØ§Ø¨Ø¹: {results['summary']['total_functions']}")
    print(f"âœ“ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {results['summary']['total_classes']}")
    
    print(f"\nâš ï¸  Ù…Ø´Ú©Ù„Ø§Øª ÛŒØ§ÙØª Ø´Ø¯Ù‡:")
    print(f"  - Ø®Ø·Ø§Ù‡Ø§ÛŒ Syntax: {len(results['issues'].get('syntax_errors', []))}")
    print(f"  - Ø®Ø·Ø§Ù‡Ø§ÛŒ Parse: {len(results['issues'].get('parse_errors', []))}")
    print(f"  - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡: {len(results['issues'].get('missing_files', []))}")
    print(f"  - ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ: {len(results['issues'].get('empty_files', []))}")
    
    print(f"\nğŸ“š ÙˆØ¶Ø¹ÛŒØª Ù…Ø³ØªÙ†Ø¯Ø§Øª:")
    print(f"  - ØªÙˆØ§Ø¨Ø¹ Ø¨Ø¯ÙˆÙ† docstring: {results['documentation_analysis']['total_undocumented_functions']}")
    print(f"  - Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† docstring: {results['documentation_analysis']['total_undocumented_classes']}")
    
    print("\n" + "="*60)

if __name__ == '__main__':
    main()
